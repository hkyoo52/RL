### Markov Process
* 미래는 오로지 현재 상태에서만 결정됨

### Markov State(=Markov chain)
* 현재 상태에서 다음 상태로 이동할때 markov process를 따름

### Markov Reward Process
* MP에서 State에서 State로 이동할때 reward에 계념을 추가한 계념

### Markov Decision Process
* Agent가 각 state마다 action을 결정하고 그 결정에 따라 reward를 받는 과정
* Action : agent가 할 행동
* Policy : state s에서 action a를 할 확률
* Transition probability : state s에서 action a를 해서 state s'로 전이할 확률
* State-value function : $V_{\pi}(s) = E_{\pi}[G_{t}|S_{t}=s]$
  * 해당 state에서 policy에 얻게 되는 reward들의 기댓값
* Action-value function : $q_{\pi}(s,a)=E_{\pi}[G_{t}|S_{t}=s, A_{t}=a]$
  * 해당 state에서 policy에 따라 action을 취했을 때 얻게되는 reward 개댓값
